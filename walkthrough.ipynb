{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSR3 Reproducibility Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook (**walkthrough.ipynb**) explains how to reproduce our results from [Sholokhov et. al. (2022): \"A Relaxation Approach to Feature Selection for Linear Mixed Effects Models\"](https://arxiv.org/abs/2205.06925?context=stat).\n",
    "\n",
    "This repository only contains the code that uses [pysr3](https://github.com/aksholokhov/pysr3) library to produce figures and tables from the paper above. To learn more about pysr3 go here: [quickstart](https://github.com/aksholokhov/pysr3), [documentation](https://aksholokhov.github.io/pysr3/), [models overview](https://aksholokhov.github.io/pysr3/models_overview.html).\n",
    "\n",
    "In our research group we take reproducibility seriously. Please open an issue in this repository if you can not reproduce our results with this notebook. We developed it using our current best knowledge on reproducibility which is imperfect and evolves as we receive more feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of today, the code works with any version of python starting 3.8.11.\n",
    "\n",
    "You need to install the following packages to your python environment to run this notebook:\n",
    "  - pysr3>=0.3.4\n",
    "  - pandas\n",
    "  - dask\n",
    "  - distributed\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "  - tqdm\n",
    "  - rpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use our *environment.yml* to create a new clean environment (**recommended**):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the lines below in your terminal\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "`conda activate msr3-paper`\n",
    "\n",
    "`python -m ipykernel install --user --name msr3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the menu above (Kernel->Change Kernel) you should be able to change your Jupyter Kernel to \"msr3\" that has all the necessary packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x7fa7e2f47600>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the results can be replicated by running **generate_all.py** with different input parameters. Let's take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pysr3 experiments [-h] [--experiments EXPERIMENTS]\n",
      "                         [--trials_from TRIALS_FROM] [--trials_to TRIALS_TO]\n",
      "                         [--use_dask USE_DASK]\n",
      "                         [--n_dask_workers N_DASK_WORKERS]\n",
      "                         [--random_seed RANDOM_SEED]\n",
      "                         [--base_folder BASE_FOLDER]\n",
      "                         [--experiment_name EXPERIMENT_NAME]\n",
      "                         [--add_timestamp ADD_TIMESTAMP]\n",
      "                         [--worker_number WORKER_NUMBER]\n",
      "                         [--draw_benchmark_data DRAW_BENCHMARK_DATA]\n",
      "                         [--verbose VERBOSE] [--num_covariates NUM_COVARIATES]\n",
      "                         [--correlation_between_adjacent_covariates CORRELATION_BETWEEN_ADJACENT_COVARIATES]\n",
      "                         [--groups_sizes GROUPS_SIZES]\n",
      "                         [--true_coefs_min TRUE_COEFS_MIN]\n",
      "                         [--true_coefs_max TRUE_COEFS_MAX]\n",
      "                         [--fit_fixed_intercept FIT_FIXED_INTERCEPT]\n",
      "                         [--fit_random_intercept FIT_RANDOM_INTERCEPT]\n",
      "                         [--obs_var OBS_VAR] [--distribution {normal,uniform}]\n",
      "                         [--elastic_eps ELASTIC_EPS]\n",
      "                         [--initializer INITIALIZER]\n",
      "                         [--logger_keys LOGGER_KEYS] [--tol_oracle TOL_ORACLE]\n",
      "                         [--tol_solver TOL_SOLVER]\n",
      "                         [--take_only_positive_part TAKE_ONLY_POSITIVE_PART]\n",
      "                         [--take_expected_value TAKE_EXPECTED_VALUE]\n",
      "                         [--max_iter_oracle MAX_ITER_ORACLE]\n",
      "                         [--max_iter_solver_pgd MAX_ITER_SOLVER_PGD]\n",
      "                         [--max_iter_solver_sr3 MAX_ITER_SOLVER_SR3]\n",
      "                         [--fixed_step_size FIXED_STEP_SIZE]\n",
      "                         [--eta_min ETA_MIN] [--eta_max ETA_MAX]\n",
      "                         [--eta_num_evals ETA_NUM_EVALS]\n",
      "                         [--information_criterion_for_model_selection INFORMATION_CRITERION_FOR_MODEL_SELECTION]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --experiments EXPERIMENTS\n",
      "                        Which experiments to run. List them as one string\n",
      "                        separated by commas, e.g. \"L0,L1\". Choices: intuition,\n",
      "                        L0, L1, ALASSO, SCAD, competitors, bullying\n",
      "  --trials_from TRIALS_FROM\n",
      "                        Each \"trial\" represents testing all algorithms listed\n",
      "                        in \"experiments\" (except intuition and bullying) on\n",
      "                        one synthetic problem. This parameter and trials_to\n",
      "                        define bounds. E.g. trials_from=1 (inclusive) and\n",
      "                        trials_to=5 (exclusive) means that all algorithms will\n",
      "                        be tested on 4 problems.\n",
      "  --trials_to TRIALS_TO\n",
      "                        Each \"trial\" represents testing all algorithms listed\n",
      "                        in \"experiments\" (except intuition and bullying) on\n",
      "                        one synthetic problem. This parameter and trials_to\n",
      "                        define bounds. E.g. trials_from=1 (inclusive) and\n",
      "                        trials_to=5 (exclusive) means that all algorithms will\n",
      "                        be tested on 4 problems.\n",
      "  --use_dask USE_DASK   Whether to use Dask Distributed to parallelize\n",
      "                        experimens. Highly recommended.\n",
      "  --n_dask_workers N_DASK_WORKERS\n",
      "                        Number of Dask workers. Defaults to the number of your\n",
      "                        CPUs - 1.\n",
      "  --random_seed RANDOM_SEED\n",
      "                        Experiments-wide random seed.\n",
      "  --base_folder BASE_FOLDER\n",
      "                        Path to the base folder (where this file is).\n",
      "  --experiment_name EXPERIMENT_NAME\n",
      "                        Name for this run. This script will create a folder\n",
      "                        named \"results/experiment_name\" where it will put all\n",
      "                        outputs of the experiments.\n",
      "  --add_timestamp ADD_TIMESTAMP\n",
      "                        Whether to add timestamp to experiment_name. Prevents\n",
      "                        overwriting your previous outputs when launching this\n",
      "                        script more than once.\n",
      "  --worker_number WORKER_NUMBER\n",
      "                        [For SLURM environment] Then number of this worker in\n",
      "                        SLURM array. Do not confuse it with n_dask_workers:\n",
      "                        the former is for parallelizing the trials over\n",
      "                        multiple nodes (e.g. on a SLURM cluster), the latter\n",
      "                        is for parallelizing experiments for one trial within\n",
      "                        one node.\n",
      "  --draw_benchmark_data DRAW_BENCHMARK_DATA\n",
      "                        Whether to produce the plots and benchmark tables\n",
      "                        after the experiments are done executing. Must be\n",
      "                        False when using SLURM.\n",
      "  --verbose VERBOSE     Whether to print log and progress messages or execute\n",
      "                        silently.\n",
      "  --num_covariates NUM_COVARIATES\n",
      "                        Number of covariates for synthetic problems\n",
      "  --correlation_between_adjacent_covariates CORRELATION_BETWEEN_ADJACENT_COVARIATES\n",
      "                        Correlations between adjacent pairs of covariates.\n",
      "  --groups_sizes GROUPS_SIZES\n",
      "                        Group sizes\n",
      "  --true_coefs_min TRUE_COEFS_MIN\n",
      "                        Magnitude of the smallest coefficient\n",
      "  --true_coefs_max TRUE_COEFS_MAX\n",
      "                        Magnitude of the largest coefficient\n",
      "  --fit_fixed_intercept FIT_FIXED_INTERCEPT\n",
      "                        Whether to add the intercept as a fixed effect\n",
      "  --fit_random_intercept FIT_RANDOM_INTERCEPT\n",
      "                        Whether to add the intercept as a random effect\n",
      "  --obs_var OBS_VAR     Variance of the observations\n",
      "  --distribution {normal,uniform}\n",
      "                        Distribution for generating features (covariates)\n",
      "  --elastic_eps ELASTIC_EPS\n",
      "                        L2 regularization coefficient to ensure convexity of\n",
      "                        the relaxed problem\n",
      "  --initializer INITIALIZER\n",
      "                        How to initialize model coefficients. Options: EM - do\n",
      "                        one iteration of EM algorithm, or None -- start with\n",
      "                        all ones.\n",
      "  --logger_keys LOGGER_KEYS\n",
      "                        Which quantities should the model record during\n",
      "                        training.\n",
      "  --tol_oracle TOL_ORACLE\n",
      "                        [For SR3] Tolerance for SR3 oracle's internal\n",
      "                        numerical subroutines\n",
      "  --tol_solver TOL_SOLVER\n",
      "                        Tolerance for the stop criterion of PGD solver\n",
      "  --take_only_positive_part TAKE_ONLY_POSITIVE_PART\n",
      "                        [For SR3] Whether to use only the positive part of the\n",
      "                        Hessian to avoid negative Hessians.\n",
      "  --take_expected_value TAKE_EXPECTED_VALUE\n",
      "                        [For SR3] Whether to use the expected value of the\n",
      "                        Hessian to avoid negative Hessians.\n",
      "  --max_iter_oracle MAX_ITER_ORACLE\n",
      "                        [For SR3] Maximum number of iterations for the SR3\n",
      "                        oracle's numerical subroutines.\n",
      "  --max_iter_solver_pgd MAX_ITER_SOLVER_PGD\n",
      "                        Maximum number of iterations for the PGD solver\n",
      "  --max_iter_solver_sr3 MAX_ITER_SOLVER_SR3\n",
      "                        [For SR3] Maximum number of iterations for the PGD\n",
      "                        solver.\n",
      "  --fixed_step_size FIXED_STEP_SIZE\n",
      "                        Size of the fixed step-size for PGD solver\n",
      "  --eta_min ETA_MIN     [For SR3] Left boundary (in Log10) for grid search\n",
      "                        over eta -- the SR3 relaxation parameter\n",
      "  --eta_max ETA_MAX     [For SR3] Right boundary (in Log10) for grid search\n",
      "                        over eta -- the SR3 relaxation parameter\n",
      "  --eta_num_evals ETA_NUM_EVALS\n",
      "                        [For SR3] Number of uniformly-sampled grid-search\n",
      "                        points for eta\n",
      "  --information_criterion_for_model_selection INFORMATION_CRITERION_FOR_MODEL_SELECTION\n",
      "                        Which information criterion to use for the final model\n",
      "                        selection. Options: jones, vaida, muller.\n"
     ]
    }
   ],
   "source": [
    "%run generate_all.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reproducibility purposes the most important ones are:\n",
    "- **experiment_name**: name for the current run. The script will create folders named 'results/experiment_name/{logs,figures,tables}' where it will place the resulting training logs, figures, and tables respectively.\n",
    "- **experiments**: which experiments to run. List them as one string separated by commas, e.g. \"L0,L1\". Choices: intuition, L0, L1, ALASSO, SCAD, competitors, bullying\n",
    "- **trials_from** and **trials_to**: how many synthetic problems do we use to compare the performance of our methods (for L0, L1, ALASSO, SCAD, and competitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intuition picture (Figure 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture can be generated via the command below. It executes in a single-thread and takes about 8 minutes on my MacBook Pro 2019. The majority of this time goes into evaluating the value function on a fine gid to draw the level-sets.\n",
    "\n",
    "The code that executes this experiment is located in **intuition.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run's input parameters are saved as: \n",
      " results/test_run/logs/experiment_inputs_1.pickle\n",
      "Generate data and fit models for the 'intuition' experiment (Figure 3)\n",
      "On a 2D sample L1 problem PGD took 1284 steps to converge and MSR3 took 18 steps to converge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating value function for betas on a grid for contour plots: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [04:18<00:00,  2.58s/it]\n",
      "Evaluating value function for gammas on a grid for contour plots: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:49<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intuition data saved as results/test_run/logs/log_intuition.pickle\n",
      "Intuition figure (Figure 2) saved as results/test_run/figures/intuition.jpg\n",
      "CPU times: user 8min 15s, sys: 2.68 s, total: 8min 18s\n",
      "Wall time: 8min 19s\n"
     ]
    }
   ],
   "source": [
    "%time %run generate_all.py --experiments \"intuition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I got:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results/test_run/figures/intuition.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment reproduces the Table 2 and Figure 4 from the paper. They compare the performance of PGD against MSR3 and MSR3-fast on a set of synthetic examples. In our paper we ran the experiments on 100 problems which took *240 CPU-hours* on our SLURM cluster. The main reason for why it takes so long is that the reference algorithms (PGD and MSR3) are quite slow: they need to be evaluated on a grid of 10 points for the regularization parameter $\\lambda$, times another 10 points for the relaxation parameter $\\eta$ for MSR3, all of that for 4 different regularizers (L0, L1, ALASSO, SCAD). With each of them taking 40 seconds on average they amount to (1+10)*10*4 = 300 minutes of CPU-time *per problem*, or trial. Our proposed algorithm MSR3-fast takes a tiny fraction (~1/400) of this time with its average execution time of 100 milliseconds.\n",
    "\n",
    "Thus, your options are:\n",
    "1. *Run it on a SLURM cluster (**preferred**)*: This is what we did. When running 100 problems in parallel the experiment took about 2 hours. You can use the provided file **run_on_slurm.bash** for it. Don't forget to adjust the username and the partition name to match your environment.\n",
    "2. *Run it on a multicore computer with Dask*: 16 workers in parallel take about 20 hours to complete the experiments on my Intel Core i9-9980K. **NB**: you may be asked to allow incoming network connections to python.\n",
    "3. *Run it on a smaller number of problems*: Running it for e.g. 3 problems won't probably give you the exact same numbers as we got in the paper due to larger variation, but it should be enough to see the overall pattern.\n",
    "4. *Run only MSR3-fast*: It should be enough if you want to benchmark only our proposed algorithm. You'll need to edit *generate_all.py* for it.\n",
    "\n",
    "**NB1**: another big reason to run the code on SLURM is resource isolation per process.\n",
    "- When you run your code in SLURM you ensure the execution time is measured correctly, as no other heavy processes are being executed in parallel.\n",
    "- When you run your code on your local machine your processes interfere with the system processes and with each other. It introduces a time-dependent noise in your measurements that is hard to predict and is nearly impossible to get rid of. In past the system process could have been assumed brief and non-influential, but nowadays modern consumer PCs run heavy background processes such as photo processing, indexing, and caching, that could take multiple cores out of your resource pool and skew the measurements for one algorithm or all of them.\n",
    "\n",
    "**NB2**: The script also saves all **datasets** as csv-files in *results/experiment_name/synthetic_data*\n",
    "\n",
    "Here is what I got in about 1 hour of execution time for (3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run's input parameters are saved as: \n",
      " results/test_run/logs/experiment_inputs_1.pickle\n",
      "\n",
      "Run comparison experiment for L0-based selection benchmark (Table 2, Figure 4). Problems to solve: [1, 2)\n",
      "Jobs to process: 21\n",
      "Processing with Dask.Distributed, with 15 workers in parallel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [14:51, 155.18s/it]"
     ]
    }
   ],
   "source": [
    "%time %run generate_all.py --experiments \"L0,L1,ALASSO,SCAD\" --trials_from 1 --trials_to 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bullying example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example (Chapter 4.2, Figure 6) we apply our method to Bullying Data from GBD. The goal of the underlying study was to estimate the burden (DALYs) of major depressive disorder (MDD) and anxiety disorders that are caused by bullying.\n",
    "\n",
    "- The dataset is located in *bullying/bullying_data.csv*\n",
    "- The code that executes the experiments and produces plots is located in *bullying/bullying_example.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment runs quickly on a single-core machine. This is how you run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run's input parameters are saved as: \n",
      " results/test_run/logs/experiment_inputs_1.pickle\n",
      "Run a feature-selection experiment on real-world data from the Bullying study. (Figure 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random feature selection saved as as results/test_run/figures/bullying_data_random_feature_selection.jpg\n",
      "Random feature selection saved as as results/test_run/figures/bullying_data_random_feature_selection.jpg\n",
      "Assessment saved as as results/test_run/figures/bullying_data_assessment_selection.jpg\n",
      "CPU times: user 7.79 s, sys: 163 ms, total: 7.96 s\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%time %run generate_all.py --experiments \"bullying\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I got:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results/test_run/figures/bullying_data_assessment_selection.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparison with alternative libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide python wrappers for four R libraries: **[fence](https://cran.r-project.org/web/packages/fence/index.html)**, **[lmmlasso](https://rdrr.io/cran/lmmlasso/)**, **[glmmlasso](https://cran.r-project.org/web/packages/glmmLasso/index.html)**, and **[pco](https://pubmed.ncbi.nlm.nih.gov/24920875/)** (see in *alterntatives/pco.r*). Each wrapper consists of two files\n",
    "- *alternatives/name.r*: an R code that runs the library *name* to the provided dataset\n",
    "- *alternatives/name_wrapper.py* a python code that links the R file above to our experimental pipeline\n",
    "\n",
    "\n",
    "To run then within our experimental pipeline you need:\n",
    "- Make sure that you have an R environment installed with the paths to it appropriately linked to the system.\n",
    "- Install the R packages for the algorithms from the list above.\n",
    "- Install rpy2 library to your python environment.\n",
    "\n",
    "As discussed  in the paper, we only report the evaluations for lmmlasso and glmmlasso because\n",
    "- PCO can not solve problems where the number of random effects (|gamma| * num_groups) exceeds the total number of objects, which is the case for our problem set.\n",
    "- Fence is too slow and runs out of memory on Macbook Pro 16 with 32GB of RAM.\n",
    "\n",
    "Nevertheless, we provide the wrappers for all four libraries. If you want to give PCO and Fence another go then you need to uncomment them in the lines 385-390 of *generate_all.py*\n",
    "\n",
    "Here is how you can launch it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run's input parameters are saved as: \n",
      " results/test_run/logs/experiment_inputs_1.pickle\n",
      "\n",
      " Run experiment for alternative libraries on L1 tasks (Table 3). Problems to solve [1, 2)\n",
      "Jobs to process: 12\n",
      "Processing with Dask.Distributed, with 15 workers in parallel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/aksh/.conda/envs/msr3-paper/lib/python3.8/site-packages/rpy2/rinterface.py:1110: UserWarning: R is not initialized by the main thread.\n",
      "                Its taking over SIGINT cannot be reversed here, and as a\n",
      "                consequence the embedded R cannot be interrupted with Ctrl-C.\n",
      "                Consider (re)setting the signal handler of your choice from\n",
      "                the main thread.\n",
      "  warnings.warn(\n",
      "R[write to console]: Loading required package: emulator\n",
      "\n",
      "R[write to console]: Loading required package: mvtnorm\n",
      "\n",
      "R[write to console]: Loading required package: miscTools\n",
      "\n",
      "R[write to console]: Loading required package: penalized\n",
      "\n",
      "R[write to console]: Loading required package: survival\n",
      "\n",
      "R[write to console]: Welcome to penalized. For extended examples, see vignette(\"penalized\").\n",
      "\n",
      "1it [00:02,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "This is a test release of the package 'lmmlasso'. If you have any questions or problems, do not hesitate to contact the author.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:04,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant covariance parameters. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:13,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n",
      "Redundant covariance parameters. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:41,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redundant covariance parameters. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/aksh/.conda/envs/msr3-paper/lib/python3.8/site-packages/rpy2/rinterface.py:1110: UserWarning: R is not initialized by the main thread.\n",
      "                Its taking over SIGINT cannot be reversed here, and as a\n",
      "                consequence the embedded R cannot be interrupted with Ctrl-C.\n",
      "                Consider (re)setting the signal handler of your choice from\n",
      "                the main thread.\n",
      "  warnings.warn(\n",
      "R[write to console]: There were 50 or more warnings (use warnings() to see the first 50)\n",
      "R[write to console]: There were 16 warnings (use warnings() to see them)\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements for competitors are saved to: \n",
      " results/test_run/logs/competitors_1.log\n",
      "The table with performance comparison for alternative libraries was saved to:results/test_run/tables/competitors_table_long.tex\n",
      "CPU times: user 10.4 s, sys: 3.77 s, total: 14.1 s\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aksh/Storage/repos/msr3-paper/competitors_table.py:79: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  table_long.to_latex(tables_folder / f\"competitors_table_long.tex\")\n"
     ]
    }
   ],
   "source": [
    "%time %run generate_all.py --experiments \"competitors\" --trials_from 1 --trials_to 2 --no_dask True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I got for running it on just one problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SR3-L1-P</th>\n",
       "      <th>glmmLasso</th>\n",
       "      <th>lmmLasso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>95 (95-95)</td>\n",
       "      <td>45 (45-45)</td>\n",
       "      <td>72 (72-72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FE Accuracy</td>\n",
       "      <td>95 (95-95)</td>\n",
       "      <td>45 (45-45)</td>\n",
       "      <td>45 (45-45)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE Accuracy</td>\n",
       "      <td>95 (95-95)</td>\n",
       "      <td>45 (45-45)</td>\n",
       "      <td>100 (100-100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>94 (94-94)</td>\n",
       "      <td>61 (61-61)</td>\n",
       "      <td>77 (77-77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FE F1</td>\n",
       "      <td>95 (95-95)</td>\n",
       "      <td>59 (59-59)</td>\n",
       "      <td>62 (62-62)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RE F1</td>\n",
       "      <td>94 (94-94)</td>\n",
       "      <td>62 (62-62)</td>\n",
       "      <td>100 (100-100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Time</td>\n",
       "      <td>0.18 (0.18-0.18)</td>\n",
       "      <td>0.57 (0.57-0.57)</td>\n",
       "      <td>6.09 (6.09-6.09)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iterations</td>\n",
       "      <td>90 (90-90)</td>\n",
       "      <td>53 (53-53)</td>\n",
       "      <td>0 (0-0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0          SR3-L1-P         glmmLasso          lmmLasso\n",
       "0     Accuracy        95 (95-95)        45 (45-45)        72 (72-72)\n",
       "1  FE Accuracy        95 (95-95)        45 (45-45)        45 (45-45)\n",
       "2  RE Accuracy        95 (95-95)        45 (45-45)     100 (100-100)\n",
       "3           F1        94 (94-94)        61 (61-61)        77 (77-77)\n",
       "4        FE F1        95 (95-95)        59 (59-59)        62 (62-62)\n",
       "5        RE F1        94 (94-94)        62 (62-62)     100 (100-100)\n",
       "6         Time  0.18 (0.18-0.18)  0.57 (0.57-0.57)  6.09 (6.09-6.09)\n",
       "7   Iterations        90 (90-90)        53 (53-53)           0 (0-0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alternatives = pd.read_csv('results/test_run/tables/competitors_table_long.csv')\n",
    "alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msr3",
   "language": "python",
   "name": "msr3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
